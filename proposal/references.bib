@article{wang2024kasa,
  title={KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models},
  author={Wang, Fan and Jiang, Juyong and Park, Chansung and Kim, Sunghun and Tang, Jing},
  journal={arXiv preprint arXiv:2412.06071},
  year={2024}
}

@inproceedings{huang2025hira,
  title={HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models},
  author={Huang, Qiushi and Ko, Tom and Zhuang, Zhan and Tang, Lilian and Zhang, Yu},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
}

@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{zhang2023adaptive,
  title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
  author={Zhang, Tianyi and Wu, Fisch and Lu, Izzeddin and Tian, Qiuyuan and Gao, Ping and Sun, Suyi and Rajani, Ruochen and Khattab, Sanjabi},
  journal={arXiv preprint arXiv:2303.10512},
  year={2023}
}

@article{kopiczko2023vera,
  title={VeRA: Vector-based Random Matrix Adaptation},
  author={Kopiczko, Dawid and Korbak, Tomasz and Stojnic, Robert and Maliszewski, Maciej and Skreta, Michał and Rittaler, Daniel and Shao, Yuhuai and Andriushchenko, Maksym and Gal, Yarin and Hoffer, Elad and Bellagente, Marco and Brekelmans, Rob and Balle, Borja and Camuto, Alexander},
  journal={arXiv preprint arXiv:2310.11454},
  year={2023}
}

@article{llm-adapters,
  title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
  author={Hu, Zhiqiang and Lee, Yihuai and Hui, Jiang and Ge, Pillay and Du, Shizhe and Tang, Liang and Li, Xiang and Li, Yankai and Wang, Hou and Xiao, Ian and Lu, Pan},
  journal={arXiv preprint arXiv:2304.01933},
  year={2023}
}

@article{meta2023llama,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{meta2023llama3,
  title={Llama 3: A More Capable, Finetuned Language Model},
  author={Meta AI},
  year={2023}
}

@article{qwen,
  title={Qwen Technical Report},
  author={Bai, Jinze and Lv, Shuai and Zhang, Fan and Li, Lei and Le, Fulong and Wang, Peng and Yang, Zhoujun and Gao, Lifeng and Cai, Keming and Zhang, You and Wu, Qihang and Chi, Zewen and Goyal, Arul and Cheng, Xuanwei and Zhao, Qingyu and Chen, Manling and Zhu, Wenhao and Tan, Bowen and Lv, Kaiming and Yu, Jin and Zhang, Qiaoyu and Bi, Yixuan and Zeng, Aojun and Liu, Jianbo and Luo, Dayiheng and Tang, Yingxuan and Zhu, Chengjie and Yao, Shengyu and Deng, Shuohuan and Ji, Wanli and Chen, Yinghan and Liu, Hongwei and Zhao, Yuan and Wu, Shanhuai and Long, Chengpeng and Lin, Lingbo and Jiang, Junlong and Zhang, Dayou and Wan, Bo and Li, Shiliang and Xu, Boer and Xie, Shuo and Wu, Hongshen and Tian, Xiangjun and Li, Junjie and Wang, Jiahao and Tang, Xiaomi and Qi, Xiang and Fu, Xiaoyong and Shi, Jinliang and Cui, Naer and Hou, Yanqi and Wang, Songyang and Zhao, Kaitian and Lin, Weize and Lian, Wei and Liu, Yanlin and Zhang, Le and Zhou, Yuanhe and Zhang, Ziwei and Hong, Yeju and Zhou, Lei and Chen, Erfei and Gan, Cong and Xiao, Tong and Tang, Hongyu and Hui, Binbin and Zhou, Yang and Ye, Song and Fan, Jingxu and Shi, Lei and Fei, Hao and Sun, Jun and Yao, Quanjin and Cong, Kaixuan and Ma, Kaifeng and Wang, Baochang and Wei, Yijia and Yuan, Bo and Xu, Hongkun and Chen, Zhongwei and Chen, Yu and Zhou, Siming and Xu, Bo and Wu, Weichao and Zhuang, Jinzhan and Zhang, Dahua and Yang, Zhenzhong and Cui, Jianwei and Yang, Borui and Lai, Huadong and Ling, Ruibo and Wu, Xiaoyin and Li, Xiaotian and Huo, Jing and Xu, Chenggang and Jia, Pengjun and Zhou, Chenfei and Zheng, Jianlin and Tao, Gabe and Wang, Xiaodong and Wang, Wenxin and Dong, Jialong and Yan, Shun and Fan, MengYuan and Gu, Huaizheng and Shi, Chuhan and Feng, Xiaohan and Qiu, Jianjian and Yang, Yangyi and Ye, Yifei and Liu, Haoming and Yan, Junteng and Zhao, Zilong and Xu, Dan and Lin, Yanwei and Zhang, Shijie and Xu, Feng and Deng, Zhiwei and Zhao, Quanming and Yao, Wei and Dolan, Quentin and Chu, Cong and Chen, Feng-Min and Lin, Yuxuan and Peng, Zhaohe and Cheng, Yu and Chen, Zewen and Shi, Jianing and Zhu, Yi and Lin, Kevin and She, Rui and Yin, Chen and Wang, Chenghu and Chen, Damai and Gao, Shei-yun and Hu, Zhen and Xiang, Shiyao and Zhang, Peng and Xue, Junyang and Teng, Zhihao and Xiao, Zifeng and Chen, Chuang and Yan, Donghua and Yang, Yuetao and Ding, Chenjuan},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{anthropic2023claude,
  title={Claude: A Conversation-Based AI Assistant},
  author={Anthropic},
  year={2023}
}

@article{he2022towards,
  title={Towards a Unified View of Parameter-Efficient Transfer Learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  journal={arXiv preprint arXiv:2110.04366},
  year={2022}
}

@article{houlsby2019parameter,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1902.00751},
  year={2019}
}

@article{li2021prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{liu2022few,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  journal={arXiv preprint arXiv:2205.05638},
  year={2022}
}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{zhang2019ernie,
  title={ERNIE: Enhanced Language Representation with Informative Entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  journal={arXiv preprint arXiv:1905.07129},
  year={2019}
}

@article{peters2019knowledge,
  title={Knowledge Enhanced Contextual Word Representations},
  author={Peters, Matthew E and Neumann, Mark and Logan IV, Robert L and Schwartz, Roy and Joshi, Vidur and Singh, Sameer and Smith, Noah A},
  journal={arXiv preprint arXiv:1909.04164},
  year={2019}
}

@article{sap2019socialiqa,
  title={Social IQA: Commonsense Reasoning about Social Interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}

@article{bisk2020piqa,
  title={PIQA: Reasoning about Physical Commonsense in Natural Language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  journal={arXiv preprint arXiv:1911.11641},
  year={2020}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2020winogrande,
  title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={arXiv preprint arXiv:1907.10641},
  year={2020}
}

@article{clark2018think,
  title={Think You Have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{mihaylov2018can,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{wolf2020transformers,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2020}
}

@article{peft,
  title={PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author={Hugging Face},
  year={2023}
} 