@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

% PEFT Methods - Adapter-based
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for {NLP}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@inproceedings{mahabadi2021compacter,
  title={Compacter: Efficient low-rank hypercomplex adapter layers},
  author={Mahabadi, Rabeeh Karimi and Henderson, James and Ruder, Sebastian},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1022--1035},
  year={2021}
}

@inproceedings{hu2021lora,
  title={{LoRA}: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

% PEFT Methods - Prompt-based
@inproceedings{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing},
  pages={4582--4597},
  year={2021}
}

@inproceedings{liu2022p,
  title={{P-tuning v2}: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  pages={61--68},
  year={2022}
}

% PEFT Methods - Sparse
@inproceedings{guo2021parameter,
  title={Parameter-efficient transfer learning with diff pruning},
  author={Guo, Demi and Rush, Alexander M and Kim, Yoon},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing},
  pages={4884--4896},
  year={2021}
}

@inproceedings{zaken2022bitfit,
  title={{BitFit}: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  pages={1--9},
  year={2022}
}

% High-rank Adaptation
@inproceedings{huang2024hira,
  title={{HiRA}: Hadamard high-rank adaptation for parameter-efficient fine-tuning},
  author={Huang, Jiawei and Chen, Lingjiao and Gu, Quanquan},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{edalati2022krona,
  title={{KronA}: Parameter efficient tuning with {K}ronecker adapter},
  author={Edalati, Ali and Tahaei, Marzieh and Rashid, Ahmad and Kobyzev, Ivan and Ghodsi, Mehrdad and Rezagholizadeh, Mehdi and Dolatabadi, Hadi Mohaghegh},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

% Knowledge-aware Methods
@inproceedings{wang2024kasa,
  title={{KaSA}: Knowledge-aware singular-value adaptation for parameter-efficient fine-tuning},
  author={Wang, Yiming and Chen, Yu and Xiong, Zhiyuan and Wang, Jinxin and Chen, Zhengwei},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{chavan2023spectral,
  title={Spectral adapters: Parameter efficient fine-tuning via spectral decomposition},
  author={Chavan, Arnav and Shen, Zhiqiang and Liu, Zhuang and Liu, Zechun and Xie, Saining and Darrell, Trevor},
  booktitle={International Conference on Computer Vision},
  pages={9175--9185},
  year={2023}
}

% Datasets - Training
@inproceedings{wang2022super,
  title={Super-{NaturalInstructions}: Generalization via declarative instructions on 1600+ {NLP} tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}

@misc{commonsense170k,
  title={Commonsense-170{K}: A comprehensive dataset for commonsense reasoning},
  author={Liu, Xiao and Wang, Hao and Li, Zhengxiao and Tang, Jie},
  year={2023},
  note={Available at: \url{https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila/Aquila-chat}}
}

% Evaluation Datasets
@inproceedings{sap2019social,
  title={Social {IQa}: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing},
  pages={4463--4473},
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={{PIQA}: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={7432--7439},
  year={2020}
}

@inproceedings{clark2019boolq,
  title={{BoolQ}: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2924--2936},
  year={2019}
}

@inproceedings{zellers2019hellaswag,
  title={{HellaSwag}: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}

@inproceedings{sakaguchi2021winogrande,
  title={{WinoGrande}: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021}
}

@inproceedings{clark2018think,
  title={Think you have solved question answering? try {ARC}, the {AI2} reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  booktitle={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}

% Base Models
@article{touvron2023llama,
  title={{Llama}: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@inproceedings{black2022gpt,
  title={{GPT-NeoX-20B}: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
  pages={95--136},
  year={2022}
}